import time


import pandas as pd
import dask.dataframe as dd
import dask.array as da
import multiprocessing
import xgboost as xgb
import lightgbm as lgb
import category_encoders as ce
import math
import gc

from dask.distributed import Client
from dask.multiprocessing import get
from dask_ml.preprocessing import Categorizer, DummyEncoder
from dask_ml.linear_model import LogisticRegression
from dask_ml.xgboost import XGBRegressor
from sklearn.svm import LinearSVC
from multiprocessing import Process
from multiprocessing import Queue

import logging
from logging import getLogger, StreamHandler, Formatter, FileHandler


"""
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Activation
#from keras.optimizers import SGD
from keras.optimizers import Adam
from keras.utils import np_utils
"""


#### common config ####
TRAINPATH = '../input/train.csv'
TESTPATH = '../input/test.csv'
CORES = multiprocessing.cpu_count()
APPLICATIONNAME = "KaggleApplication"

#### log config ####

logger = getLogger(APPLICATIONNAME)
logger.setLevel(logging.DEBUG)
handler_format = Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

stream_handler = StreamHandler()
stream_handler.setLevel(logging.DEBUG)
stream_handler.setFormatter(handler_format)
logger.addHandler(stream_handler)

file_handler = FileHandler('/var/log/'+APPLICATIONNAME+'.log')
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(handler_format)
logger.addHandler(file_handler)



"""
#### keras config ####
NB_EPOCH = 20
BATCH_SIZE = 100
VERBOSE = 1
NB_CLASSES = 1
#OPTIMIZER = SGD()
OPTIMIZER = Adam(lr=1e-4)
N_HIDDEN = 128
VALIDATION_SPLIT = 0.2
"""


def readcsv(path):
    try:
        data = pd.read_csv(path)
        return data
    except Exception as e:
        print(e)
        logger.error(e)
        return False

def readcsv_chunk(path):
    dtyp = {'AVProductStatesIdentifier':'int32','AVProductsEnabled':'int8','AVProductsInstalled':'int8','DefaultBrowsersIdentifier':'int16','OsBuild':'int16','Wdft_IsGamer':'int8', 'OsSuite':'int16'}
    try:
        data = pd.read_csv(path, chunksize=50)
        return data
    except Exception as e:
        logger.error(e)
        return False

def dummy(data, col):
    split_01 = data[col].str.split('.', expand=True)
    split_01.columns = [col+"_1", col+"_2", col+"_3", col+"_4"]
    sp_01 = split_01[col+"_1"] + split_01[col+"_2"]
    sp_02 = split_01[col+"_1"] + split_01[col+"_2"] + split_01[col+"_3"]
    sp_03 = split_01[col+"_1"] + split_01[col+"_2"] + split_01[col+"_3"] + split_01[col+"_4"]
    data = data.drop(col, axis=1)
    return pd.concat([data, pd.DataFrame(sp_01, columns=[col+"_1"], dtype="category"), pd.DataFrame(sp_02, columns=[col+"_2"], dtype="category"), pd.DataFrame(sp_03, columns=[col+"_3"], dtype="category")], axis=1)


def dummy02(data, col):
    split_01 = data[col].str.split('.', expand=True)
    split_01.columns = [col+"_1", col+"_2", col+"_3", col+"_4"]
    sp_01 = split_01[col+"_1"] + split_01[col+"_2"]
    sp_02 = split_01[col+"_1"] + split_01[col+"_2"] + split_01[col+"_3"]
    data = data.drop(col, axis=1)
    return pd.concat([data, pd.DataFrame(sp_01, columns=[col+"_1"], dtype="category"), pd.DataFrame(sp_02, columns=[col+"_2"], dtype="category")], axis=1)


def selfSVM(q, x_train, y_train, x_test, y_test, predict_data):
    ##### SVM #####
    logger.info("create SVM model...")
    clf = LinearSVC(C=0.00158, loss='squared_hinge', penalty='l2', dual=True, tol=1e-4)
    clf.fit(x_train, y_train)
    logger.info(clf.score(x_test, y_test))
    logger.info("SVM score is "+str(clf.score(x_test, y_test)))
    q.put(pd.DataFrame(clf.predict(predict_data)))


def selfxgboost(q, x_train, y_train, x_test, y_test, predict_data):
    ##### xgboost #####
    logger.info("create xgboost model...")
    gbm = xgb.XGBClassifier(
     #learning_rate = 0.02,
     n_estimators= 2000,
     max_depth= 4,
     min_child_weight= 2,
     gamma=0.9,                        
     subsample=0.8,
     colsample_bytree=0.8,
     objective= 'binary:logistic',
     scale_pos_weight=1).fit(x_train, y_train)
    logger.info("xgboost score is "+str(gbm.score(x_test, y_test)))
    q.put(pd.DataFrame(gbm.predict(predict_data)))

"""
def selfneuralnetwork(x_train, y_train, x_test, y_test, predict_data):
    #### neural network ####
    model = Sequential()
    model.add(Dense(N_HIDDEN, input_shape=(x_train.shape[1], )))
    model.add(Activation('relu'))
    model.add(Dense(N_HIDDEN))
    model.add(Activation('relu'))
    model.add(Dense(NB_CLASSES))
    model.add(Activation('softmax'))
    model.summary()

    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=OPTIMIZER,
                  metrics=['accuracy'])

    model.fit(x_train, y_train,batch_size=BATCH_SIZE,epochs=NB_EPOCH,verbose=VERBOSE,validation_split=VALIDATION_SPLIT)
    print(model.score(x_test, y_test))
"""
def split_list(l, n):
    for idx in range(0, len(l), n):
        yield l[idx:idx + n]

def dummyMultiprocess(q, cat_columns, all_data_pd):
    ce_ohe = ce.OneHotEncoder(cols=cat_columns,handle_unknown='ignore')
    q.put(ce_ohe.fit_transform(all_data_pd))


def main():
    ###### check environment #####
    logger.info("number of cores are "+str(CORES))


    ###### import data #####
    ### import all data ###
    logger.info("import all data...")
    #train = readcsv(TRAINPATH)
    #test = readcsv(TESTPATH)
    

    ### import chunk data ###
    logger.info("import chunk of data...")
    n_train_samples = 1000
    n_test_samples = 100
    train_chunk = readcsv_chunk(TRAINPATH)
    train = train_chunk.get_chunk(n_train_samples)
    test_chunk = readcsv_chunk(TESTPATH)
    test = test_chunk.get_chunk(n_test_samples)

    logger.info("trian shape is "+str(train.shape))
    logger.info("test shape is "+str(test.shape))
    

    dtype = {'AVProductStatesIdentifier':'category','AVProductsEnabled':'float16','AVProductsInstalled':'float16','OsBuild':'category','Wdft_IsGamer':'float16', 'OsSuite':'category', 'Firewall':'float16', 'HasDetections':'float16', 'HasTpm':'float16', 'IsSxsPassiveMode':'float16', 'AppVersion':'category','AvSigVersion':'category','Census_ActivationChannel':'category','Census_GenuineStateName':'category','Census_OSVersion':'category','Census_OSWUAutoUpdateOptionsName':'category','EngineVersion':'category','OsPlatformSubRelease':'category','OsVer':'category','Processor':'category','ProductName':'category','SmartScreen':'category', 'SkuEdition':'category', 'Platform':'category'}
    #dtype = {'AVProductStatesIdentifier':'float32','AVProductsEnabled':'float16','AVProductsInstalled':'float16','OsBuild':'float16','Wdft_IsGamer':'float16', 'OsSuite':'float16', 'Firewall':'float16', 'HasDetections':'float16', 'HasTpm':'float16', 'IsSxsPassiveMode':'float16', 'AppVersion':'category','AvSigVersion':'category','Census_ActivationChannel':'category','Census_GenuineStateName':'category','Census_OSVersion':'category','Census_OSWUAutoUpdateOptionsName':'category','EngineVersion':'category','OsPlatformSubRelease':'category','OsVer':'category','Processor':'category','ProductName':'category','SmartScreen':'category'}
    all_data_pd = pd.concat([train, test], ignore_index=True, sort=True)
    all_data_pd = all_data_pd.astype(dtype)
    all_data_pd.reindex(index=range(all_data_pd.shape[0]))


    ##### transfar data to dask dataframe #####
    logger.info("transfar data to dask dataframe...")
    all_data_dd = dd.from_pandas(all_data_pd, npartitions=CORES)
    logger.info('partition numberï¼š' + str(all_data_dd.npartitions))


    ###### remove columns #####
    logger.info("remove columns...")
    rem_columns = ['MachineIdentifier','IsBeta','RtpStateBitfield','CountryIdentifier','CityIdentifier','OrganizationIdentifier','GeoNameIdentifier','LocaleEnglishNameIdentifier','OsBuildLab','IsProtected','AutoSampleOptIn','PuaMode','SMode','IeVerIdentifier','UacLuaenable','Census_DeviceFamily','Census_MDC2FormFactor','Census_OEMNameIdentifier','Census_OEMModelIdentifier','Census_ProcessorCoreCount','Census_ProcessorManufacturerIdentifier','Census_ProcessorModelIdentifier','Census_ProcessorClass','Census_PrimaryDiskTotalCapacity','Census_PrimaryDiskTypeName','Census_SystemVolumeTotalCapacity','Census_HasOpticalDiskDrive','Census_TotalPhysicalRAM','Census_ChassisTypeName','Census_InternalPrimaryDiagonalDisplaySizeInInches','Census_InternalPrimaryDisplayResolutionHorizontal','Census_InternalPrimaryDisplayResolutionVertical','Census_PowerPlatformRoleName','Census_InternalBatteryType','Census_InternalBatteryNumberOfCharges','Census_OSArchitecture','Census_OSBranch','Census_OSBuildNumber','Census_OSBuildRevision','Census_OSEdition','Census_OSSkuName','Census_OSInstallTypeName','Census_OSInstallLanguageIdentifier','Census_OSUILocaleIdentifier','Census_IsPortableOperatingSystem','Census_IsFlightingInternal','Census_IsFlightsDisabled','Census_FlightRing','Census_ThresholdOptIn','Census_FirmwareManufacturerIdentifier','Census_IsSecureBootEnabled','Census_IsWIMBootEnabled','Census_IsVirtualDevice','Census_IsTouchEnabled','Census_IsPenCapable','Census_IsAlwaysOnAlwaysConnectedCapable','Wdft_RegionIdentifier', 'Census_FirmwareVersionIdentifier', 'DefaultBrowsersIdentifier']
    all_data_dd = all_data_dd.drop(rem_columns, axis=1)
    logger.info(all_data_dd.shape)
    del train, train_chunk
    gc.collect()


    ##### fill NaN #####
    logger.info("fill NaN...")
    int_columns = all_data_dd.select_dtypes(include=int).columns
    flo_columns = all_data_dd.select_dtypes(include=float).columns
    flo16_columns = all_data_dd.select_dtypes(include=['float16']).columns
    flo32_columns = all_data_dd.select_dtypes(include=['float32']).columns
    cat_columns = all_data_dd.select_dtypes(include="category").columns
    obj_columns = all_data_dd.select_dtypes(include=object).columns
    for col in int_columns:
        all_data_dd[col] = all_data_dd[col].fillna(all_data_pd[col].median())
    for col in flo_columns:
        all_data_dd[col] = all_data_dd[col].fillna(all_data_pd[col].median())
    for col in flo16_columns:
        all_data_dd[col] = all_data_dd[col].fillna(all_data_pd[col].median())
    for col in flo32_columns:
        all_data_dd[col] = all_data_dd[col].fillna(all_data_pd[col].median())
    for col in cat_columns:
        all_data_dd[col] = all_data_dd[col].fillna(all_data_pd[col].mode()[0])
    for col in obj_columns:
        all_data_dd[col] = all_data_dd[col].fillna(all_data_pd[col].mode()[0])
    del int_columns, flo_columns, flo16_columns, flo32_columns, cat_columns, obj_columns
    gc.collect()


    ##### split data #####
    logger.info("split data...")
    all_data_pd = all_data_dd.compute()
    all_data_pd_col_bef = all_data_pd.columns
    all_data_pd = dummy02(all_data_pd, "AppVersion")
    all_data_pd = dummy(all_data_pd, "Census_OSVersion")
    all_data_pd = dummy02(all_data_pd, "EngineVersion")
    all_data_pd = dummy02(all_data_pd, "AvSigVersion")
    all_data_pd_col_aft = all_data_pd.columns

    list_ab = list(set(all_data_pd_col_aft) - set(all_data_pd_col_bef))
    del all_data_dd
    gc.collect()


    start = time.time()
    """
    ##### dummy encoding01 ##### 0.20468592643737793
    print("dummy encoding01...")
    de = DummyEncoder()
    all_data_pd = de.fit_transform(all_data_pd)
    """

    #### dummy encoding02 #####  10.903063774108887
    logger.info("dummy encoding...")
    cat_columns = list(all_data_pd.select_dtypes(include="category").columns)
    result = list(split_list(cat_columns, math.ceil(len(cat_columns) / CORES)))
    q = list(range(CORES))
    p = list(range(CORES))
    for i in range(CORES):
        q[i] = Queue()
        p[i] = Process(target=dummyMultiprocess, args=(q[i], result[i], all_data_pd[result[i]]))
        p[i].start()
        all_data_pd = all_data_pd.drop(result[i], axis=1)
    for i in range(CORES):
        all_data_pd = pd.concat([all_data_pd, q[i].get()], axis=1)
    for i in range(CORES):
        p[i].join()

    """
    #### 14.318004846572876
    cat_columns = list(all_data_pd.select_dtypes(include="category").columns)
    ce_ohe = ce.OneHotEncoder(cols=cat_columns,handle_unknown='ignore')
    all_data_pd = ce_ohe.fit_transform(all_data_pd)
    """



    logger.info("====================================================================================================")
    logger.info(time.time() - start)
    logger.info("====================================================================================================")


    ##### split data to train and test #####
    logger.info("split data to train and test...")
    all_data_pd.to_csv("../output/all_data_pd.csv", index=False, header=False)
    all_data_dd = all_data_pd
    predict_data = all_data_dd.drop("HasDetections", axis=1)[n_train_samples:]
    all_data_dd = all_data_dd[:n_train_samples]
    train_tmp = all_data_dd[:int(n_train_samples*0.8)]
    test_tmp = all_data_dd[int(n_train_samples*0.8):]
    x_train = train_tmp.drop("HasDetections", axis=1)
    x_test =  test_tmp.drop("HasDetections", axis=1)
    y_train = train_tmp["HasDetections"]
    y_test = test_tmp["HasDetections"]

    logger.info(x_train.shape)
    logger.info(x_test.shape)
    logger.info(y_train.shape)
    logger.info(y_test.shape)


    clf_q = Queue()
    xgb_q = Queue()

    clf_p = Process(target=selfSVM, args=(clf_q, x_train, y_train, x_test, y_test, predict_data))
    xgb_p = Process(target=selfxgboost, args=(xgb_q, x_train, y_train, x_test, y_test, predict_data))

    logger.info("clf Process started.")
    clf_p.start()
    logger.info("xgb Process started.")
    xgb_p.start()

    result = pd.concat([test["MachineIdentifier"], clf_q.get()],  ignore_index=True, axis=1)
    result.to_csv("../output/clf_result.csv", index=False, header=False)

    result = pd.concat([test["MachineIdentifier"], xgb_q.get()],  ignore_index=True, axis=1)
    result.to_csv("../output/xgb_result.csv", index=False, header=False)


    logger.info("clf Process finished.")
    clf_p.join()
    logger.info("xgb Process finished.")
    xgb_p.join()


if __name__ == '__main__':
	main()

