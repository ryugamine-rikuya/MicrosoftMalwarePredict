import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
#from sklearn import svm, metrics, preprocessing, cross_validation
#from sklearn import svm, metrics, preprocessing

from sklearn.model_selection import GridSearchCV
# svmのインポート
#from sklearn import svm
#from sklearn.svm import SVC
from sklearn.svm import LinearSVC

from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.naive_bayes import GaussianNB as GNB
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.ensemble import BaggingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import VotingClassifier


from sklearn.model_selection import ShuffleSplit

from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)
import xgboost as xgb
import lightgbm as lgb


# %matplotlib inline

TRAINPATH = "../input/train.csv"
TESTPATH = "../input/test.csv"

def readcsv(path):
    
    try:
        data = pd.read_csv(path)
        print(data.columns)
        return data
    except Exception as e:
        pritn(e)
        return False

def readcsv_chunk(path):
    dtyp = {'AVProductStatesIdentifier':'int32','AVProductsEnabled':'int8','AVProductsInstalled':'int8','DefaultBrowsersIdentifier':'int16','OsBuild':'int16','Wdft_IsGamer':'int8', 'OsSuite':'int16'}
    try:
        data = pd.read_csv(path, chunksize=50)
        return data
    except Exception as e:
        print(e)
        return False

def stringtransfer(data, colmun):
    data_cat = data[colmun]
    return data_cat.factorize()

def stringTransfar(data, colmuns):
    for col in colmuns:
        _data_cat = data[col]
        _cat_encoded, categories = _data_cat.factorize()
        data[col] = _cat_encoded
    return data

def transfar1hot(_data, colmuns):
    encoder = OneHotEncoder()
    for col in colmuns:
        _data_cat = _data[col]
        _cat_encoded, categories = _data_cat.factorize()
        _data_cat_1hot = encoder.fit_transform(_cat_encoded.reshape(-1,1))
        for category in range(len(categories)):
            col_name = str(col)+"_"+str(categories[category])
            _data[col_name] = _data_cat_1hot.toarray()[:,category]
            _data[col_name] =  _data.astype({col_name:'float32'})
        _data = _data.drop(col, axis=1)
    return _data

def makeHistObjGraph(data, colmuns):
    for col in colmuns:
        tmp, tmp_cat = stringtransfer(data, col)
        tmp01 = tmp[data["HasDetections"] == 0]
        tmp02 = tmp[data["HasDetections"] == 1]
        sns.set()
        sns.set_style('whitegrid')
        sns.set_palette('Set1')
        fig = plt.figure()
        ax = fig.add_subplot(1, 1, 1)
        ax.hist([tmp01, tmp02], bins=50)
        if len(tmp_cat.tolist()) < 15:
            plt.text( 0.99, 0.99, '\n'.join(tmp_cat.tolist()),horizontalalignment='right', verticalalignment='top', family='monospace', transform=ax.transAxes)
        plt.title(col+' Box plot')
        plt.xlabel("HasDetections")
        plt.ylabel(col)
        plt.grid()
        plt.savefig( '../graph/hist/'+col+'.png' )
        plt.close(fig)

def makeBoxGraph(data, colmuns):
    for col in colmuns:
        tmp01 = data[col][data["HasDetections"] == 0]
        tmp02 = data[col][data["HasDetections"] == 1]
        points = (tmp01, tmp02)
        # 箱ひげ図
        fig, ax = plt.subplots()
        bp = ax.boxplot(points)
        ax.set_xticklabels([col+'_0', col+'_1'])
        plt.title(col+' Box plot')
        plt.xlabel("HasDetections")
        plt.ylabel(col)
        # Y軸のメモリのrange
        #plt.ylim([train_head[col].min(),train_head[col].max()])
        plt.grid()
        plt.savefig( '../graph/box/'+col+'.png' )
        plt.close(fig)

def splitAddColumns(data, colmuns):
    for col in colmuns:
        if col != "AvSigVersion":
            df = data[col].str.split('.', expand=True)
            new_df_0 = transfar1hot(pd.DataFrame(df[0].values.tolist(), columns=[col+'_0']).astype({col+'_0':'float16'}), [col+'_0'])
            new_df_1 = transfar1hot(pd.DataFrame((df[0]+df[1]).values.tolist(), columns=[col+'_1']).astype({col+'_1':'float16'}), [col+'_1'])
            new_df_2 = transfar1hot(pd.DataFrame((df[0]+df[1]+df[2]).values.tolist(), columns=[col+'_2']).astype({col+'_2':'float32'}), [col+'_2'])
            new_df_3 = transfar1hot(pd.DataFrame((df[0]+df[1]+df[2]+df[3]).values.tolist(), columns=[col+'_3']).astype({col+'_3':'float64'}), [col+'_3'])
            data = pd.concat([data, new_df_0, new_df_1, new_df_2, new_df_3], axis=1)
            data = data.drop(col, axis=1)
        else:
            df = data[col].str.split('.', expand=True)
            new_df_0 = transfar1hot(pd.DataFrame(df[0].values.tolist(), columns=[col+'_0']).astype({col+'_0':'float16'}), [col+'_0'])
            new_df_1 = transfar1hot(pd.DataFrame((df[0]+df[1]).values.tolist(), columns=[col+'_1']).astype({col+'_1':'float16'}), [col+'_1'])
            new_df_2 = transfar1hot(pd.DataFrame((df[0]+df[1]+df[2]).values.tolist(), columns=[col+'_2']).astype({col+'_2':'float32'}), [col+'_2'])
            data = pd.concat([data, new_df_0, new_df_1, new_df_2], axis=1)
            data = data.drop(col, axis=1)
    return data



class MyPreprocessor(object):
    def __init__(self, remove_col):
        pass
    def fit(self, X, y):
        """学習フェーズ"""
        return self
    def transform(self, X):
        """変換フェーズ"""
        # 2 で割った余りを返す
        return X % 2
    def predict(self, X):
        """分類フェーズ"""
        # 何もせずに素通しする
        return X



if __name__ == '__main__':
    n_samples = 100
    #n_samples = 8921483

    #train = readcsv(TRAINPATH)
    print("data import...")
    #test = readcsv(TESTPATH)
    
    
    train_chunk = readcsv_chunk(TRAINPATH)
    test_chunk = readcsv_chunk(TESTPATH)
    train = train_chunk.get_chunk(n_samples)
    test = test_chunk.get_chunk(n_samples)
    all_data = pd.concat([train, test], ignore_index=True, sort=True)

    # 欠損値の補完
    print("data fill NaN...")
    int_columns = all_data.select_dtypes(include=int).columns
    flo_columns = all_data.select_dtypes(include=float).columns
    obj_columns = all_data.select_dtypes(include=object).columns
    for col in int_columns:
        all_data[col] = all_data[col].fillna(all_data[col].median())
    for col in flo_columns:
        all_data[col] = all_data[col].fillna(all_data[col].median())
    for col in obj_columns:
        all_data[col] = all_data[col].fillna(all_data[col].mode()[0])

    dtyp = {'AVProductStatesIdentifier':'float32','AVProductsEnabled':'float16','AVProductsInstalled':'float16','DefaultBrowsersIdentifier':'float32','OsBuild':'float16','Wdft_IsGamer':'float16', 'OsSuite':'float16', 'Firewall':'float16', 'HasDetections':'float16', 'HasTpm':'float16', 'IsSxsPassiveMode':'float16', 'AppVersion':'category','AvSigVersion':'category','Census_ActivationChannel':'category','Census_GenuineStateName':'category','Census_OSVersion':'category','Census_OSWUAutoUpdateOptionsName':'category','EngineVersion':'category','OsPlatformSubRelease':'category','OsVer':'category','Processor':'category','ProductName':'category','SmartScreen':'category'}

    all_data = all_data.astype(dtyp)

    remove_01 = ['MachineIdentifier','IsBeta','RtpStateBitfield','CountryIdentifier','CityIdentifier','OrganizationIdentifier','GeoNameIdentifier','LocaleEnglishNameIdentifier','OsBuildLab','IsProtected','AutoSampleOptIn','PuaMode','SMode','IeVerIdentifier','UacLuaenable','Census_DeviceFamily','Census_MDC2FormFactor','Census_OEMNameIdentifier','Census_OEMModelIdentifier','Census_ProcessorCoreCount','Census_ProcessorManufacturerIdentifier','Census_ProcessorModelIdentifier','Census_ProcessorClass','Census_PrimaryDiskTotalCapacity','Census_PrimaryDiskTypeName','Census_SystemVolumeTotalCapacity','Census_HasOpticalDiskDrive','Census_TotalPhysicalRAM','Census_ChassisTypeName','Census_InternalPrimaryDiagonalDisplaySizeInInches','Census_InternalPrimaryDisplayResolutionHorizontal','Census_InternalPrimaryDisplayResolutionVertical','Census_PowerPlatformRoleName','Census_InternalBatteryType','Census_InternalBatteryNumberOfCharges','Census_OSArchitecture','Census_OSBranch','Census_OSBuildNumber','Census_OSBuildRevision','Census_OSEdition','Census_OSSkuName','Census_OSInstallTypeName','Census_OSInstallLanguageIdentifier','Census_OSUILocaleIdentifier','Census_IsPortableOperatingSystem','Census_IsFlightingInternal','Census_IsFlightsDisabled','Census_FlightRing','Census_ThresholdOptIn','Census_FirmwareManufacturerIdentifier','Census_IsSecureBootEnabled','Census_IsWIMBootEnabled','Census_IsVirtualDevice','Census_IsTouchEnabled','Census_IsPenCapable','Census_IsAlwaysOnAlwaysConnectedCapable','Wdft_RegionIdentifier', 'Census_FirmwareVersionIdentifier']
    remove_02 = ['Platform','SkuEdition'] #処理をした後に削除
    make_new_col = ['Platform','SkuEdition']
    split = ['EngineVersion','AppVersion','AvSigVersion','Census_OSVersion'] # split後に続けてone hot
    one_hot_01 = ['ProductName','DefaultBrowsersIdentifier','AVProductStatesIdentifier','Processor','OsVer','OsBuild','OsSuite','OsPlatformSubRelease','SmartScreen','Census_OSWUAutoUpdateOptionsName','Census_GenuineStateName','Census_ActivationChannel']
    one_hot_02 = ['Platform_SkuEdition']
    change = ['AVProductsInstalled', 'AVProductsEnabled'] # 1→0, 2→1 全ての値に対して1を引く

    # 不要なカラムを削除
    print("data drop...")
    all_data = all_data.drop(remove_01, axis=1)

    # カテゴリ値をsplitして新しいカテゴリの作成
    print("data split...")
    all_data = splitAddColumns(all_data, split)

    # カテゴリデータをone hotカラムに変換
    print("data to one hot...")
    all_data = transfar1hot(all_data, one_hot_01)

    # カテゴリ同士の結合
    print("create to new columns...")
    all_data['Platform_SkuEdition'] = all_data['Platform'] + '_' + all_data['SkuEdition']
    # 不要なカラムを削除
    all_data = all_data.drop(remove_02, axis=1)
    all_data = transfar1hot(all_data, one_hot_02)

    for col in change:
        all_data[col] = all_data[col] - 1

    # カラムを数値に変換
    # 欠損値の補完
    print("data fill NaN...")
    flo_columns = all_data.select_dtypes(include=float).columns
    for col in flo_columns:
        all_data[col] = all_data[col].fillna(all_data[col].median())

    print("===============")

    pre_test = all_data[train.shape[0]:].astype('float32').drop("HasDetections", axis=1)

    all_data = all_data[:train.shape[0]].astype('float32')

    y_train = all_data["HasDetections"][:int(train.shape[0]*0.8)].astype('float32')
    y_test = all_data["HasDetections"][int(train.shape[0]*0.8):].astype('float32')
    x_train = all_data.drop("HasDetections", axis=1)[:int(train.shape[0]*0.8)].astype('float32')
    x_test = all_data.drop("HasDetections", axis=1)[int(train.shape[0]*0.8):].astype('float32')
    



    # SVM
    print("create SVM model...")
    clf = LinearSVC(C=0.00158, loss='squared_hinge', penalty='l2', dual=True, tol=1e-4)
    clf.fit(x_train, y_train)
    
    result = pd.concat([test["MachineIdentifier"], pd.DataFrame(clf.predict(pre_test))],  ignore_index=True, axis=1)
    result.to_csv("clf_result.csv", index=False, header=False)


    
    print("===============")
    # xgboost
    print("create xgboost model...")
    gbm = xgb.XGBClassifier(
     #learning_rate = 0.02,
     n_estimators= 2000,
     max_depth= 4,
     min_child_weight= 2,
     gamma=0.9,                        
     subsample=0.8,
     colsample_bytree=0.8,
     objective= 'binary:logistic',
     scale_pos_weight=1).fit(x_train, y_train)
    print(gbm.score(x_test, y_test))
    result = pd.concat([test["MachineIdentifier"], pd.DataFrame(gbm.predict(pre_test))],  ignore_index=True, axis=1)
    result.to_csv("xgb_result.csv", index=False, header=False)
    


    print("===============")
    # LightGBM
    print("create lightgbm model...")
    lgbm_params = {
        # 多値分類問題
        'objective': 'multiclass',
        # クラス数は 3
        'num_class': 2,
    }


    lgb_train = lgb.Dataset(x_train, y_train)
    lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)

    #model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval)
    model = lgb.train(lgbm_params, lgb_train, num_boost_round=18)
    y_pred = model.predict(pre_test, num_iteration=model.best_iteration)
    y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする
    #print(y_pred)

    # 精度 (Accuracy) を計算する
    #accuracy = sum(y_test == y_pred_max) / len(y_test)
    #print(accuracy)
    result = pd.concat([test["MachineIdentifier"], pd.DataFrame(y_pred_max)],  ignore_index=True, axis=1)
    result.to_csv("light_result.csv", index=False, header=False)



    """
    cv_results = lgb.cv(lgbm_params, lgb_train, nfold=10)
    print(cv_results)
    cv_logloss = cv_results['multi_logloss-mean']
    round_n = np.arange(len(cv_logloss))

    plt.xlabel('round')
    plt.ylabel('logloss')
    plt.plot(round_n, cv_logloss)
    plt.show()
    """


    """
    lgb_train = lgb.Dataset(x_train, y_train)
    lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)

    model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval)
    y_pred = model.predict(x_test, num_iteration=model.best_iteration)
    y_pred_max = np.argmax(y_pred, axis=1)  # 最尤と判断したクラスの値にする

    # 精度 (Accuracy) を計算する
    accuracy = sum(y_test == y_pred_max) / len(y_test)
    print(accuracy)
    """

    """ xbgoost学習
    gbm = xgb.XGBClassifier(
     #learning_rate = 0.02,
     n_estimators= 2000,
     max_depth= 4,
     min_child_weight= 2,
     gamma=0.9,                        
     subsample=0.8,
     colsample_bytree=0.8,
     objective= 'binary:logistic',
     #nthread= -1,
     scale_pos_weight=1).fit(x_train, y_train)
    print(gbm.score(x_test, y_test))
    result = pd.concat([test["MachineIdentifier"], pd.DataFrame(gbm.predict(pre_test))],  ignore_index=True, axis=1)

    result.to_csv("result.csv", index=False, header=False)
    """ 




    """"
    svm =LinearSVC(C=0.00158, loss='squared_hinge', penalty='l2', dual=True, tol=1e-4)
    #svm =SVC(C=0.00158, tol=1e-4)
    lr = LogisticRegression()
    knn = KNN(n_jobs=-1)
    nb = GNB()
    rfc = RFC(n_estimators=500, n_jobs=-1)
    bgg = BaggingClassifier(n_estimators=300, n_jobs=-1)
    mlp = MLPClassifier(hidden_layer_sizes=(40, 20), max_iter=1000)

    estimators = list(zip(["svm","lr","knn","nb","rfc","bgg","mlp"],
                          [svm,lr,knn,nb,rfc,bgg,mlp]))

    for name, clf in estimators:
        clf.fit(x_train, y_train)
        print("================================")
        print(name)
        print(clf.score(x_test, y_test))
    """


    """
    for v in ["hard", "soft"]:
        vc_hard = VotingClassifier(estimators, voting=v, probability=False)
        vc_hard.fit(x_train, y_train)
        print("================================")
        print(v, "voting")
        print(vc_hard.score(x_test, y_test))

    clf = SVC(gamma='auto')
    clf.fit(x_train, y_train)
    print (clf.score(x_test, y_test))
    """


    # モデルの種類
    # 決定木
    # ロジスティック回帰
    # ニューラルネット
    # svm
    # xxx
    # これらのアンサンブル手法を用いる


